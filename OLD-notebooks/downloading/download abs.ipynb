{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for unzipping\n",
    "import requests, zipfile\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# for the good shit\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATIVE_PATH = \"../../data/landing/\"\n",
    "SA2_CODE_NAME = \"SA2 code\"\n",
    "MERGE_COLUMNS = [\"SA2_MAINCODE_2016\", \"SA2_CODE_2021\", SA2_CODE_NAME, \"year\"]\n",
    "MERGE_FILE_NAME = \"merge.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating File system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories to create\n",
    "DIRECTORIES = [\"ABS\", \"ABS/2021\", \"ABS/2016\"]\n",
    "directory_paths = [RELATIVE_PATH + dir for dir in DIRECTORIES]\n",
    "\n",
    "# create the paths\n",
    "for directory_path in directory_paths:\n",
    "    os.makedirs(directory_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def delete_contents(directory):\n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(directory):\n",
    "        # Clear the contents of the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove files and symlinks\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove directories\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading started for 2021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed\n",
      "Finishing extracting\n",
      "Downloading started for 2016\n",
      "Download completed\n",
      "Finishing extracting\n"
     ]
    }
   ],
   "source": [
    "# get the request \n",
    "urls = [\"https://www.abs.gov.au/census/find-census-data/datapacks/download/2021_GCP_SA2_for_VIC_short-header.zip\",\n",
    "        \"https://www.abs.gov.au/census/find-census-data/datapacks/download/2016_GCP_SA2_for_VIC_short-header.zip\"]\n",
    "names = [\"2021\", \"2016\"]\n",
    "\n",
    "# donwload and unzip each url\n",
    "for url, name in zip(urls, names):\n",
    "    print(f\"Downloading started for {name}\")\n",
    "    response = requests.get(url)\n",
    "    print(\"Download completed\")\n",
    "\n",
    "    # prepare the outfile\n",
    "    out_path = RELATIVE_PATH + \"ABS/\" + name\n",
    "    delete_contents(out_path)\n",
    "\n",
    "    # extract the zip file\n",
    "    zipfile_obj = zipfile.ZipFile(BytesIO(response.content))\n",
    "    zipfile_obj.extractall(out_path)\n",
    "\n",
    "    print(\"Finishing extracting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME the data file\n",
    "paths = [RELATIVE_PATH + \"ABS/\" + name + \"/\" for name in names]\n",
    "\n",
    "for path in paths:\n",
    "    # find the data file\n",
    "    data_names = [dir for dir in os.listdir(path) if \"Census GCP\" in dir]\n",
    "\n",
    "    if (len(data_names) != 1):\n",
    "        print(\"something wrong, the old path doesn't exist\")\n",
    "        break\n",
    "\n",
    "    # out path and in path\n",
    "    curr_path = path + data_names[0]\n",
    "    new_path = path + \"data\"\n",
    "\n",
    "    # rename the file\n",
    "    os.rename(curr_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME the meta data file\n",
    "paths = [RELATIVE_PATH + \"ABS/\" + name + \"/Metadata/\" for name in names]\n",
    "\n",
    "for path in paths:\n",
    "    # find the data file\n",
    "    data_names = [dir for dir in os.listdir(path) if \"Metadata\" in dir]\n",
    "\n",
    "    if (len(data_names) != 1):\n",
    "        print(\"something wrong, the old path doesn't exist\")\n",
    "        break\n",
    "\n",
    "    # out path and in path\n",
    "    curr_path = path + data_names[0]\n",
    "    new_path = path + \"column_names.xlsx\"\n",
    "\n",
    "    # rename the file\n",
    "    os.rename(curr_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_merge(new_df, final_df):\n",
    "    if (final_df.empty):\n",
    "        return new_df\n",
    "\n",
    "    # get the attributes used for the merge\n",
    "    merge_columns = list(set(new_df.columns) & set(MERGE_COLUMNS) & set(final_df.columns))\n",
    "\n",
    "    # get the columns not already in the data frame\n",
    "    new_columns = list(set(new_df.columns) - set(final_df.columns) - set(merge_columns))\n",
    "\n",
    "    # merge columns\n",
    "    merged_df = pd.merge(final_df, new_df[merge_columns + new_columns], on=merge_columns, how=\"inner\")\n",
    "\n",
    "    # check if any rows lost\n",
    "    print(f\"lost {final_df.shape[0] - merged_df.shape[0]} out of {final_df.shape[0]} records\")\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_df(df_list):\n",
    "    # keep on aggregating\n",
    "    agg_df = df_list[0]\n",
    "    for new_df in df_list[1:]:\n",
    "        agg_df = get_next_merge(new_df, agg_df)\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Note: uses the actual index name\"\"\"\n",
    "def rename_dict(df, csv_dict):\n",
    "    # find the merge columns present in the dictionary\n",
    "    merge_columns = list(set(df.columns) & set(MERGE_COLUMNS))\n",
    "\n",
    "    if (len(merge_columns) != 1):\n",
    "        print(\"more than one merge column in dictionary\")\n",
    "        return None\n",
    "\n",
    "    # filter the dictionary with the old names\n",
    "    df = df[merge_columns + list(csv_dict[\"rename\"].keys())]\n",
    "\n",
    "    # get the new names\n",
    "    df.columns = [SA2_CODE_NAME] + [csv_dict[\"name\"] + \": \" + new_name for new_name in  csv_dict[\"rename\"].values()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(csv_dict):\n",
    "    # get the relative directory all files\n",
    "    relative_path = RELATIVE_PATH + 'ABS/' + csv_dict[\"year\"] + \"/data/\"\n",
    "\n",
    "    # get all the files from the directory with the code\n",
    "    all_files = os.listdir(RELATIVE_PATH + 'ABS/' + csv_dict[\"year\"] + \"/data\")\n",
    "    code_paths = [relative_path + file for file in all_files if csv_dict[\"code\"] in file]\n",
    "\n",
    "    # create a new dataframe for each path\n",
    "    code_dataframes = []\n",
    "    for code_path in code_paths:\n",
    "        code_dataframes.append(pd.read_csv(code_path))\n",
    "\n",
    "    # merge the dictionary\n",
    "    out_df = get_merged_df(code_dataframes)\n",
    "\n",
    "    # rename the columns\n",
    "    out_df = rename_dict(out_df, csv_dict)\n",
    "\n",
    "    # add a year if necessary\n",
    "    if (csv_dict.get(\"add year\")):\n",
    "        out_df[\"year\"] = int(csv_dict[\"year\"])\n",
    "\n",
    "        # write the csv\n",
    "        out_df.to_csv(RELATIVE_PATH + \"ABS/\" + csv_dict[\"name\"] + \"_\" + csv_dict[\"year\"] + \".csv\")\n",
    "    else:\n",
    "        # write csv with year code\n",
    "        out_df.to_csv(RELATIVE_PATH + \"ABS/\" + csv_dict[\"name\"] + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(code, metadata, start=False, ending=False):\n",
    "    # filter for the code\n",
    "    metadata = metadata[metadata[\"Profiletable\"].str.startswith(code)]\n",
    "\n",
    "    # if table is empty, raise a warning\n",
    "    if (metadata.empty):\n",
    "        print(f\"no corresponding code to {code}\")\n",
    "        return\n",
    "    \n",
    "    # find the mask for the values of interest\n",
    "    if (ending):\n",
    "        select_mask = metadata[\"Long\"].apply(lambda x: x[-len(ending):] == ending)\n",
    "        metadata = metadata[select_mask][[\"Short\", \"Long\"]]\n",
    "    if (start):\n",
    "        select_mask = metadata[\"Long\"].str.startswith(start)\n",
    "        metadata = metadata[select_mask][[\"Short\", \"Long\"]]\n",
    "\n",
    "    # create the out dict, and convert to lower\n",
    "    out_dict = {short: long.lower() for short, long in metadata.values}\n",
    "\n",
    "    # create a dictionary\n",
    "    return out_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16984, 5)\n",
      "Short                                object\n",
      "Long                                 object\n",
      "DataPackfile                         object\n",
      "Profiletable                         object\n",
      "Columnheadingdescriptioninprofile    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Short</th>\n",
       "      <th>Long</th>\n",
       "      <th>DataPackfile</th>\n",
       "      <th>Profiletable</th>\n",
       "      <th>Columnheadingdescriptioninprofile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sequential</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>G1</th>\n",
       "      <td>Tot_P_M</td>\n",
       "      <td>Total_Persons_Males</td>\n",
       "      <td>G01</td>\n",
       "      <td>G01</td>\n",
       "      <td>Males</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2</th>\n",
       "      <td>Tot_P_F</td>\n",
       "      <td>Total_Persons_Females</td>\n",
       "      <td>G01</td>\n",
       "      <td>G01</td>\n",
       "      <td>Females</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G3</th>\n",
       "      <td>Tot_P_P</td>\n",
       "      <td>Total_Persons_Persons</td>\n",
       "      <td>G01</td>\n",
       "      <td>G01</td>\n",
       "      <td>Persons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G4</th>\n",
       "      <td>Age_0_4_yr_M</td>\n",
       "      <td>Age_groups_0_4_years_Males</td>\n",
       "      <td>G01</td>\n",
       "      <td>G01</td>\n",
       "      <td>Males</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G5</th>\n",
       "      <td>Age_0_4_yr_F</td>\n",
       "      <td>Age_groups_0_4_years_Females</td>\n",
       "      <td>G01</td>\n",
       "      <td>G01</td>\n",
       "      <td>Females</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Short                          Long DataPackfile  \\\n",
       "Sequential                                                            \n",
       "G1               Tot_P_M           Total_Persons_Males          G01   \n",
       "G2               Tot_P_F         Total_Persons_Females          G01   \n",
       "G3               Tot_P_P         Total_Persons_Persons          G01   \n",
       "G4          Age_0_4_yr_M    Age_groups_0_4_years_Males          G01   \n",
       "G5          Age_0_4_yr_F  Age_groups_0_4_years_Females          G01   \n",
       "\n",
       "           Profiletable Columnheadingdescriptioninprofile  \n",
       "Sequential                                                 \n",
       "G1                  G01                             Males  \n",
       "G2                  G01                           Females  \n",
       "G3                  G01                           Persons  \n",
       "G4                  G01                             Males  \n",
       "G5                  G01                           Females  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_excel(RELATIVE_PATH + \"ABS/2021/Metadata/column_names.xlsx\",\n",
    "                         sheet_name=\"Cell Descriptors Information\",\n",
    "                         skiprows = list(range(10)),\n",
    "                         header=0,\n",
    "                         index_col=0)\n",
    "\n",
    "print(metadata.shape)\n",
    "print(metadata.dtypes)\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_RELATIONSHIPS_2021 = {\n",
    "    \"year\": \"2021\",\n",
    "    \"code\": \"G27\",\n",
    "    \"name\": \"relationships\",\n",
    "    \"rename\": {\n",
    "        \"P_Ptn_in_RM_Tot\": \"married\",\n",
    "        \"P_Ptn_in_DFM_Tot\": \"defacto\",\n",
    "        \"P_LonePnt_Tot\": \"lone parents\",\n",
    "        \"P_CU15_Tot\": \"child under 15\",\n",
    "        \"P_DpStu_Tot\": \"dependent student\",\n",
    "        \"P_NDpChl_Tot\": \"non dependent child\",\n",
    "        \"P_OthRI_Tot\": \"other related individual\",\n",
    "        \"P_GrpH_Mem_Tot\": \"group household\",\n",
    "        \"P_LonePsn_Tot\": \"lone persons\"\n",
    "    },\n",
    "    \"add year\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lost 0 out of 524 records\n"
     ]
    }
   ],
   "source": [
    "write_csv(CSV_RELATIONSHIPS_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_RELATIONSHIPS_2016 = {\n",
    "    \"year\": \"2016\",\n",
    "    \"code\": \"G23\",\n",
    "    \"name\": \"relationships\",\n",
    "    \"rename\": {\n",
    "        \"P_H_or_W_in_RM_Tot\": \"married\",\n",
    "        \"P_Ptn_in_DFM_Tot\": \"defacto\",\n",
    "        \"P_LonePnt_Tot\": \"lone parents\",\n",
    "        \"P_CU15_Tot\": \"child under 15\",\n",
    "        \"P_DpStu_Tot\": \"dependent student\",\n",
    "        \"P_NDpChl_Tot\": \"non dependent child\",\n",
    "        \"P_OthRI_Tot\": \"other related individual\",\n",
    "        \"P_GrpH_Mem_Tot\": \"group household\",\n",
    "        \"P_LonePsn_Tot\": \"lone persons\"\n",
    "    },\n",
    "    \"add year\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_csv(CSV_RELATIONSHIPS_2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overseas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_OVERSEAS = {\n",
    "    \"year\": \"2021\",\n",
    "    \"code\": \"G45\",\n",
    "    \"name\": \"overseas\",\n",
    "    \"rename\": {\"Difnt_Usl_add_5_yr_ago_OS_P\": \"5 years\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(CSV_OVERSEAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_dict = get_columns(\"G09\", metadata, start=\"PERSONS\", ending=\"Total\")\n",
    "\n",
    "# get the keys and values\n",
    "birth_dict_keys = birth_dict.keys()\n",
    "birth_dict_values = birth_dict.values()\n",
    "\n",
    "# remove the start and end\n",
    "birth_dict_values = [\" \".join(ethnic_string.split(\"_\")[1:-1]) for ethnic_string in birth_dict_values]\n",
    "\n",
    "# rerecreate the dict\n",
    "birth_dict = {key: value for key, value in zip(birth_dict_keys, birth_dict_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_BIRTH = {\n",
    "    \"year\": \"2021\",\n",
    "    \"code\": \"G09\",\n",
    "    \"name\": \"birth\",\n",
    "    \"rename\": birth_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lost 0 out of 524 records\n",
      "lost 0 out of 524 records\n",
      "lost 0 out of 524 records\n",
      "lost 0 out of 524 records\n",
      "lost 0 out of 524 records\n",
      "lost 0 out of 524 records\n",
      "lost 0 out of 524 records\n"
     ]
    }
   ],
   "source": [
    "write_csv(CSV_BIRTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Studying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_STUDYING = {\n",
    "    \"year\": \"2021\",\n",
    "    \"code\": \"G15\",\n",
    "    \"name\": \"studying\",\n",
    "    \"rename\": {\n",
    "        \"Preschool_P\": \"preschool\",\n",
    "        \"Primary_Government_P\": \"primary government\",\n",
    "        \"Primary_Catholic_P\": \"primary catholic\",\n",
    "        \"Primry_Othr_non_Govt_P\": \"primary other\",\n",
    "        \"Primary_Tot_Primary_P\": \"primary total\",\n",
    "        \"Secondary_Government_P\": \"secondary government\",\n",
    "        \"Secondary_Catholic_P\": \"secondary catholic\",\n",
    "        \"Secondary_Tot_Secondary_P\": \"secondary total\",\n",
    "        \"Tert_Voc_edu_Tot_P\": \"tafe total\",\n",
    "        \"Tert_Uni_oth_h_edu_Ft_15_24_P\": \"tertiary FT 14-25\",\n",
    "        \"Tert_Uni_oth_h_edu_Ft_25_ov_P\": \"tertiary FT 25+\",\n",
    "        \"Tert_Uni_oth_h_edu_Pt_15_24_P\": \"tertiary PT 14-25\",\n",
    "        \"Tert_Uni_oth_h_edu_Pt_25_ov_P\": \"tertiary PT 25+\",\n",
    "        \"Tert_Uni_other_high_edu_Tot_P\": \"tertiary total\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(CSV_STUDYING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lost 0 out of 524 records\n",
      "lost 0 out of 524 records\n",
      "lost 0 out of 524 records\n"
     ]
    }
   ],
   "source": [
    "# get all the csvs\n",
    "merge_path = RELATIVE_PATH + \"ABS/\"\n",
    "merging_files = [merge_path + file for file in os.listdir(merge_path) \n",
    "                 if (file != MERGE_FILE_NAME) and (file.endswith(\".csv\"))\n",
    "                 and (not file.endswith(\"_2016.csv\") and (not file.endswith(\"_2021.csv\")))]\n",
    "\n",
    "# get the list of dataframes\n",
    "merging_frames = []\n",
    "for file in merging_files:\n",
    "    merging_frames.append(pd.read_csv(file))\n",
    "\n",
    "get_merged_df(merging_frames).to_csv(merge_path + MERGE_FILE_NAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
